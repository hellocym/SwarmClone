import asyncio
import os
import torch
import openai
from dataclasses import dataclass, field
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer
)
from uuid import uuid4
from contextlib import AsyncExitStack
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.types import Tool
import time
import random
from typing import Any
from .modules import *
from .messages import *
from .utils import *

@dataclass
class LLMConfig(ModuleConfig):
    chat_maxsize: int = field(default=20, metadata={
        "required": False,
        "desc": "å¼¹å¹•æ¥å—æ•°é‡ä¸Šé™",
        "min": 1,  # æœ€å°‘æ¥å— 1 æ¡å¼¹å¹•
        "max": 1000
    })
    chat_size_threshold: int = field(default=10, metadata={
        "required": False,
        "desc": "å¼¹å¹•é€æ¡å›å¤æ•°é‡ä¸Šé™",
        "min": 1,  # æœ€å°‘é€æ¡å›å¤ 1 æ¡
        "max": 100
    })
    do_start_topic: bool = field(default=False, metadata={
        "required": False,
        "desc": "æ˜¯å¦è‡ªåŠ¨å‘èµ·å¯¹è¯"
    })
    idle_timeout: int | float = field(default=120, metadata={
        "required": False,
        "desc": "è‡ªåŠ¨å‘èµ·å¯¹è¯æ—¶é—´é—´éš”",
        "min": 0.0,
        "max": 600,
        "step": 1.0  # æ­¥é•¿ä¸º 1
    })
    asr_timeout: int = field(default=60, metadata={
        "required": False,
        "desc": "è¯­éŸ³è¯†åˆ«è¶…æ—¶æ—¶é—´",
        "min": 1,  # æœ€å°‘ 1 ç§’
        "max": 3600  # æœ€å¤§ 1 å°æ—¶
    })
    tts_timeout: int = field(default=60, metadata={
        "required": False,
        "desc": "è¯­éŸ³åˆæˆè¶…æ—¶æ—¶é—´",
        "min": 1,  # æœ€å°‘ 1 ç§’
        "max": 3600  # æœ€å¤§ 1 å°æ—¶
    })
    chat_role: str = field(default="user", metadata={
        "required": False,
        "desc": "å¼¹å¹•å¯¹åº”çš„èŠå¤©è§’è‰²"
    })
    asr_role: str = field(default="user", metadata={
        "required": False,
        "desc": "è¯­éŸ³è¾“å…¥å¯¹åº”çš„èŠå¤©è§’è‰²"
    })
    chat_template: str = field(default="{user}: {content}", metadata={
        "required": False,
        "desc": "å¼¹å¹•çš„æç¤ºè¯æ¨¡æ¿"
    })
    asr_template: str = field(default="{user}: {content}", metadata={
        "required": False,
        "desc": "è¯­éŸ³è¾“å…¥æç¤ºè¯æ¨¡æ¿"
    })
    system_prompt: str = field(default="""ä½ æ˜¯ä¸€åªçŒ«å¨˜""", metadata={
        "required": False,
        "desc": "ç³»ç»Ÿæç¤ºè¯",
        "multiline": True
    })  # TODOï¼šæ›´å¥½çš„ç³»ç»Ÿæç¤ºã€MCPæ”¯æŒ
    mcp_support: bool = field(default=False, metadata={
        "required": False,
        "desc": "æ˜¯å¦æ”¯æŒ MCP"
    })
    mcp_path1: str = field(default="", metadata={
        "required": False,
        "desc": "MCP è·¯å¾„ 1 (è¯·æŒ‡å‘ MCP è„šæœ¬ï¼Œä»¥ .py æˆ– .js ç»“å°¾ï¼Œä»…æ”¯æŒ stdio äº¤äº’æ–¹å¼)"
    })
    mcp_path2: str = field(default="", metadata={
        "required": False,
        "desc": "MCP è·¯å¾„ 2"
    })
    mcp_path3: str = field(default="", metadata={
        "required": False,
        "desc": "MCP è·¯å¾„ 3"
    })
    classifier_model_path: str = field(default="~/.swarmclone/llm/EmotionClassification/SWCBiLSTM", metadata={
        "required": False,
        "desc": "æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹è·¯å¾„"
    })
    classifier_model_id: str = field(default="MomoiaMoia/SWCBiLSTM", metadata={
        "required": False,
        "desc": "æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹id"
    })
    classifier_model_source: str = field(default="modelscope", metadata={
        "required": False,
        "desc": "æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹æ¥æºï¼Œä»…æ”¯æŒhuggingfaceæˆ–modelscope",
        "selection": True,
        "options": [
            {"key": "HuggingfaceğŸ¤—", "value": "huggingface"},
            {"key": "ModelScope", "value": "modelscope"}
        ]
    })
    model_id: str = field(default="", metadata={
        "required": True,
        "desc": "æ¨¡å‹id"
    })
    model_url: str = field(default="", metadata={
        "required": True,
        "desc": "æ¨¡å‹apiç½‘å€"
    })
    api_key: str = field(default="", metadata={
        "required": True,
        "desc": "api key",
        "password": True
    })
    temperature: float = field(default=0.7, metadata={
        "required": False,
        "desc": "æ¨¡å‹æ¸©åº¦",
        "selection": False,
        "options": [
            {"key": "0.7", "value": 0.7},
            {"key": "0.9", "value": 0.9},
            {"key": "1.0", "value": 1.0}
        ],
        "min": 0.0,  # æœ€å°æ¸©åº¦ä¸º 0
        "max": 1.0,  # æœ€å¤§æ¸©åº¦è®¾ä¸º 1
        "step": 0.1  # æ­¥é•¿ä¸º 0.1
    })

class LLM(ModuleBase):
    role: ModuleRoles = ModuleRoles.LLM
    config_class = LLMConfig
    config: config_class
    def __init__(self, config: config_class | None = None, **kwargs):
        super().__init__(config, **kwargs)
        self.state: LLMState = LLMState.IDLE
        self.history: list[dict[str, str]] = []
        self.generated_text: str = ""
        self.generate_task: asyncio.Task[Any] | None = None
        self.chat_maxsize: int = self.config.chat_maxsize
        self.chat_size_threshold: int = self.config.chat_size_threshold
        self.chat_queue: asyncio.Queue[ChatMessage] = asyncio.Queue(maxsize=self.chat_maxsize)
        self.do_start_topic: bool = self.config.do_start_topic
        self.idle_timeout: int | float = self.config.idle_timeout
        self.asr_timeout: int = self.config.asr_timeout
        self.tts_timeout: int = self.config.tts_timeout
        self.idle_start_time: float = time.time()
        self.waiting4asr_start_time: float = time.time()
        self.waiting4tts_start_time: float = time.time()
        self.asr_counter = 0 # æœ‰å¤šå°‘äººåœ¨è¯´è¯ï¼Ÿ
        self.about_to_sing = False # æ˜¯å¦å‡†å¤‡æ’­æ”¾æ­Œæ›²ï¼Ÿ
        self.song_id: str = ""
        self.chat_role = self.config.chat_role
        self.asr_role = self.config.asr_role
        self.chat_template = self.config.chat_template
        self.asr_template = self.config.asr_template
        if self.config.system_prompt:
            self._add_system_history(self.config.system_prompt)
        self.mcp_sessions: list[ClientSession] = []
        self.tools: list[list[Tool]] = []
        self.exit_stack = AsyncExitStack()
        abs_classifier_path = os.path.expanduser(self.config.classifier_model_path)
        successful = False
        while not successful: # åŠ è½½æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹
            try:
                print(f"æ­£åœ¨ä»{abs_classifier_path}åŠ è½½æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹â€¦â€¦")
                classifier_model = AutoModelForSequenceClassification.from_pretrained(
                    abs_classifier_path,
                    torch_dtype="auto",
                    trust_remote_code=True
                ).to("cpu")
                classifier_tokenizer = AutoTokenizer.from_pretrained(
                    abs_classifier_path,
                    padding_side="left",
                    trust_remote_code=True
                )
                successful = True
                self.classifier_model = classifier_model
                self.classifier_tokenizer = classifier_tokenizer
            except Exception:
                download_model(
                    self.config.classifier_model_id,
                    self.config.classifier_model_source,
                    abs_classifier_path
                )
        
        self.model_id = self.config.model_id
        self.client = openai.AsyncOpenAI(
            api_key=self.config.api_key,
            base_url=self.config.model_url
        )
        self.temperature = self.config.temperature

    async def init_mcp(self):
        available_servers = filter(lambda x: bool(x), [self.config.mcp_path1, self.config.mcp_path2, self.config.mcp_path3])
        for server in available_servers:
            is_python = server.endswith('.py')
            is_js = server.endswith('.js')
            if not (is_python or is_js):
                continue
            command = 'python' if is_python else 'node'
            server_params = StdioServerParameters(
                command=command,
                args=[server],
            )
            stdio, write = await self.exit_stack.enter_async_context(stdio_client(server_params))
            session = await self.exit_stack.enter_async_context(ClientSession(stdio, write))
            await session.initialize()
            tools: list[Tool] = (await session.list_tools()).tools
            self.tools.append(tools)
            self.mcp_sessions.append(session)
    
    def _switch_to_generating(self):
        self.state = LLMState.GENERATING
        self.generated_text = ""
        self.generate_task = asyncio.create_task(self.start_generating())
    
    def _switch_to_waiting4asr(self):
        if self.generate_task is not None and not self.generate_task.done():
            self.generate_task.cancel()
        if self.generated_text:
            self._add_llm_history(self.generated_text)
        self.generated_text = ""
        self.generate_task = None
        self.state = LLMState.WAITING4ASR
        self.waiting4asr_start_time = time.time()
        self.asr_counter = 1 # ç­‰å¾…ç¬¬ä¸€ä¸ªäºº
    
    def _switch_to_idle(self):
        self.state = LLMState.IDLE
        self.idle_start_time = time.time()
    
    def _switch_to_waiting4tts(self):
        self._add_llm_history(self.generated_text)
        self.generated_text = ""
        self.generate_task = None
        self.state = LLMState.WAITING4TTS
        self.waiting4tts_start_time = time.time()
    
    def _switch_to_singing(self):
        self.state = LLMState.SINGING
        self.about_to_sing = False
        self._add_system_history(f'ä½ å”±äº†ä¸€é¦–åä¸º{self.song_id}çš„æ­Œã€‚')

    def _add_history(self, role: str, content: str, template: str | None = None, user: str | None = None):
        """ç»Ÿä¸€çš„å†å²æ·»åŠ æ–¹æ³•"""
        if template and user:
            formatted_content = template.format(user=user, content=content)
        else:
            formatted_content = content
        self.history.append({'role': role, 'content': formatted_content})

    def _add_chat_history(self, user: str, content: str):
        self._add_history(self.chat_role, content, self.chat_template, user)
    
    def _add_asr_history(self, user: str, content: str):
        self._add_history(self.asr_role, content, self.asr_template, user)
    
    def _add_llm_history(self, content: str):
        self._add_history('assistant', content)
    
    def _add_system_history(self, content: str):
        self._add_history('system', content)
   
    async def run(self):
        if self.config.mcp_support:
            await self.init_mcp()
        while True:
            try:
                task = self.task_queue.get_nowait()
                print(self.state, task)
            except asyncio.QueueEmpty:
                task = None
            
            if isinstance(task, ChatMessage):
                # è‹¥å°äºä¸€å®šé˜ˆå€¼åˆ™å›å¤æ¯ä¸€æ¡ä¿¡æ¯ï¼Œè‹¥è¶…è¿‡åˆ™é€æ¸é™ä½å›å¤æ¦‚ç‡
                if (qsize := self.chat_queue.qsize()) < self.chat_size_threshold:
                    prob = 1
                else:
                    prob = 1 - (qsize - self.chat_size_threshold) / (self.chat_maxsize - self.chat_size_threshold)
                if random.random() < prob:
                    try:
                        self.chat_queue.put_nowait(task)
                    except asyncio.QueueFull:
                        pass
            if isinstance(task, SongInfo):
                self.about_to_sing = True
                self.song_id = task.get_value(self)["song_id"]

            match self.state:
                case LLMState.IDLE:
                    if isinstance(task, ASRActivated):
                        self._switch_to_waiting4asr()
                    elif self.about_to_sing:
                        await self.results_queue.put(
                            ReadyToSing(self, self.song_id)
                        )
                        self._switch_to_singing()
                    elif not self.chat_queue.empty():
                        try:
                            chat = self.chat_queue.get_nowait().get_value(self) # é€æ¡å›å¤å¼¹å¹•
                            self._add_chat_history(chat['user'], chat['content']) ## TODOï¼šå¯èƒ½éœ€è¦ä¸€æ¬¡å›å¤å¤šæ¡å¼¹å¹•
                            self._switch_to_generating()
                        except asyncio.QueueEmpty:
                            pass
                    elif self.do_start_topic and time.time() - self.idle_start_time > self.idle_timeout:
                        self._add_system_history("è¯·éšä¾¿è¯´ç‚¹ä»€ä¹ˆå§ï¼")
                        self._switch_to_generating()

                case LLMState.GENERATING:
                    if isinstance(task, ASRActivated):
                        self._switch_to_waiting4asr()
                    if self.generate_task is not None and self.generate_task.done():
                        self._switch_to_waiting4tts()

                case LLMState.WAITING4ASR:
                    if time.time() - self.waiting4asr_start_time > self.asr_timeout:
                        self._switch_to_idle() # ASRè¶…æ—¶ï¼Œå›åˆ°å¾…æœº
                    if isinstance(task, ASRMessage):
                        message_value = task.get_value(self)
                        speaker_name = message_value["speaker_name"]
                        content = message_value["message"]
                        self._add_asr_history(speaker_name, content)
                        self.asr_counter -= 1 # æœ‰äººè¯´è¯å®Œæ¯•ï¼Œè®¡æ•°å™¨-1
                    if isinstance(task, ASRActivated):
                        self.asr_counter += 1 # æœ‰äººå¼€å§‹è¯´è¯ï¼Œè®¡æ•°å™¨+1
                    if self.asr_counter <= 0: # æ‰€æœ‰äººè¯´è¯å®Œæ¯•ï¼Œå¼€å§‹ç”Ÿæˆ
                        self._switch_to_generating()

                case LLMState.WAITING4TTS:
                    if time.time() - self.waiting4tts_start_time > self.tts_timeout:
                        self._switch_to_idle() # å¤ªä¹…æ²¡æœ‰TTSå®Œæˆä¿¡æ¯ï¼Œè¯´æ˜TTSç”Ÿæˆå¤±è´¥ï¼Œå›åˆ°å¾…æœº
                    if isinstance(task, AudioFinished):
                        self._switch_to_idle()
                    elif isinstance(task, ASRActivated):
                        self._switch_to_waiting4asr()
                
                case LLMState.SINGING:
                    if isinstance(task, FinishedSinging):
                        self._switch_to_idle()

            await asyncio.sleep(0.1) # é¿å…å¡æ­»äº‹ä»¶å¾ªç¯
    
    async def start_generating(self) -> None:
        iterator = self.iter_sentences_emotions()
        try:
            async for sentence, emotion in iterator:
                self.generated_text += sentence
                await self.results_queue.put(
                    LLMMessage(
                        self,
                        sentence,
                        str(uuid4()),
                        emotion
                    )
                )
        except asyncio.CancelledError:
            await iterator.aclose()
        finally:
            await self.results_queue.put(LLMEOS(self))
    
    @torch.no_grad()
    async def get_emotion(self, text: str) -> dict[str, float]:
        print(text)
        labels = ['neutral', 'like', 'sad', 'disgust', 'anger', 'happy']
        ids = self.classifier_tokenizer([text], return_tensors="pt")['input_ids']
        probs = (
            (await asyncio.to_thread(self.classifier_model, input_ids=ids))
            .logits
            .softmax(dim=-1)
            .squeeze()
        )
        return dict(zip(labels, probs.tolist()))
    
    def dict2message(self, message: dict[str, Any]):
        from openai.types.chat import (
            ChatCompletionUserMessageParam,
            ChatCompletionAssistantMessageParam,
            ChatCompletionSystemMessageParam,
            ChatCompletionToolMessageParam
        )
        
        match message:
            case {'role': 'user', 'content': content}:
                return ChatCompletionUserMessageParam(role="user", content=str(content))
            case {'role': 'assistant', 'content': content, **rest}:
                return ChatCompletionAssistantMessageParam(
                    role="assistant", 
                    content=str(content),
                    tool_calls=rest.get('tool_calls') or []
                )
            case {'role': 'system', 'content': content}:
                return ChatCompletionSystemMessageParam(role="system", content=str(content))
            case {'role': 'tool', 'content': content, 'tool_call_id': tool_call_id}:
                return ChatCompletionToolMessageParam(
                    role="tool", 
                    content=str(content),
                    tool_call_id=str(tool_call_id)
                )
            case _:
                raise ValueError(f"Invalid message: {message}")

    def get_mcp_tools(self):
        ## By: Claude Code (Powered by Kimi-K2)
        """è·å–æ‰€æœ‰å¯ç”¨çš„MCPå·¥å…·"""
        if not self.config.mcp_support:
            return []
        
        return [
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema
                }
            }
            for tool_list in self.tools
            for tool in tool_list
        ]

    async def execute_mcp_tool(self, tool_name: str, arguments):
        ## By: Claude Code (Powered by Kimi-K2)
        """æ‰§è¡ŒæŒ‡å®šçš„MCPå·¥å…·è°ƒç”¨"""
        if not self.config.mcp_support:
            raise ValueError("MCP support is not enabled")
        
        # æŸ¥æ‰¾å¯¹åº”çš„å·¥å…·ä¼šè¯
        for session_idx, tool_list in enumerate(self.tools):
            for tool in tool_list:
                if tool.name == tool_name:
                    session = self.mcp_sessions[session_idx]
                    try:
                        result = await session.call_tool(tool_name, arguments)
                        # å°† CallToolResult è½¬ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸æ ¼å¼
                        if hasattr(result, 'content'):
                            # å¤„ç† CallToolResult å¯¹è±¡
                            content_list = []
                            for content_item in result.content:
                                if hasattr(content_item, 'text'):
                                    content_list.append({"type": "text", "text": content_item.text})
                                elif hasattr(content_item, 'type') and hasattr(content_item, 'data'):
                                    content_list.append({"type": content_item.type, "data": content_item.data})
                            return {"content": content_list}
                        else:
                            # å¤„ç†å…¶ä»–æ ¼å¼çš„ç»“æœ
                            return {"content": [{"type": "text", "text": str(result)}]}
                    except Exception as e:
                        print(f"Error executing MCP tool {tool_name}: {e}")
                        return {"error": str(e)}
        
        raise ValueError(f"Tool {tool_name} not found")
    
    async def _generate_with_tools_stream(self, messages, available_tools):
        ## By: Claude Code (Powered by Kimi-K2)
        """æµå¼æ¨¡å¼ï¼šä½¿ç”¨å·¥å…·è¿›è¡Œå¯¹è¯ç”Ÿæˆçš„è¾…åŠ©æ–¹æ³•"""
        try:
            request_params = {
                "model": self.model_id,
                "messages": messages,
                "stream": True,
                "temperature": self.temperature
            }
            
            if available_tools:
                request_params["tools"] = available_tools
                request_params["tool_choice"] = "auto"
            
            response_stream = await self.client.chat.completions.create(**request_params)
            tool_calls_accumulator = {}
            
            async for chunk in response_stream:
                if chunk.choices and chunk.choices[0].delta:
                    delta = chunk.choices[0].delta
                    content = delta.content or ""
                    tool_calls_output = []
                    
                    if delta.tool_calls:
                        for tool_call_delta in delta.tool_calls:
                            index = tool_call_delta.index
                            if index not in tool_calls_accumulator:
                                tool_calls_accumulator[index] = {
                                    "id": "",
                                    "type": "function",
                                    "function": {
                                        "name": "",
                                        "arguments": ""
                                    }
                                }
                            
                            # ç´¯ç§¯å·¥å…·è°ƒç”¨ä¿¡æ¯
                            if tool_call_delta.id:
                                tool_calls_accumulator[index]["id"] = tool_call_delta.id
                            if tool_call_delta.function and tool_call_delta.function.name:
                                tool_calls_accumulator[index]["function"]["name"] = tool_call_delta.function.name
                            if tool_call_delta.function and tool_call_delta.function.arguments:
                                tool_calls_accumulator[index]["function"]["arguments"] += tool_call_delta.function.arguments
                    
                    # åªæœ‰åœ¨æµç»“æŸæ—¶æ‰è¾“å‡ºå®Œæ•´çš„å·¥å…·è°ƒç”¨
                    finish_reason = chunk.choices[0].finish_reason
                    if finish_reason == "tool_calls":
                        tool_calls_output = list(tool_calls_accumulator.values())
                    
                    yield {
                        "content": content,
                        "tool_calls": tool_calls_output,
                        "finish_reason": finish_reason
                    }
        except Exception as e:
            print(f"Error in _generate_with_tools_stream: {e}")
            yield {
                "content": f"æŠ±æ­‰ï¼Œç”Ÿæˆå›å¤æ—¶å‡ºç°é”™è¯¯: {e}",
                "tool_calls": [],
                "finish_reason": "stop"
            }

    async def iter_sentences_emotions(self):
        ## By: KyvYang + Claude Code (Powered by Kimi-K2)
        generating_sentence = ""
        try:
            # è·å–å¯ç”¨çš„MCPå·¥å…·
            available_tools = self.get_mcp_tools()
            
            # åˆ›å»ºæ¶ˆæ¯å†å²
            current_messages = [self.dict2message(message) for message in self.history]
            
            # å¾ªç¯å¤„ç†å·¥å…·è°ƒç”¨ï¼Œç›´åˆ°æ²¡æœ‰æ›´å¤šå·¥å…·è°ƒç”¨
            while True:
                # ä½¿ç”¨æµå¼APIç”Ÿæˆå“åº”
                accumulated_content = ""
                tool_calls_buffer = []
                
                async for chunk in self._generate_with_tools_stream(current_messages, available_tools):
                    content = chunk["content"] or ""
                    tool_calls = chunk["tool_calls"]
                    finish_reason = chunk["finish_reason"]
                    
                    # ç´¯ç§¯å†…å®¹
                    accumulated_content += str(content)
                    
                    # å¤„ç†å†…å®¹æµ
                    if content and not tool_calls_buffer:  # æ²¡æœ‰å¾…å¤„ç†çš„å·¥å…·è°ƒç”¨
                        generating_sentence += str(content)
                        self.generated_text += str(content)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„å¥å­å¯ä»¥å‘é€
                        sentences = split_text(generating_sentence)
                        if sentences[:-1]:
                            for sentence in sentences[:-1]:
                                if sentence.strip():
                                    yield sentence.strip(), await self.get_emotion(sentence.strip())
                            generating_sentence = sentences[-1]
                    
                    # æ”¶é›†å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆåœ¨æµç»“æŸæ—¶å¤„ç†ï¼‰
                    if tool_calls:
                        tool_calls_buffer.extend(tool_calls)
                    
                    # æµç»“æŸå¤„ç†
                    if finish_reason == "stop" or finish_reason == "tool_calls":
                        break
                
                # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
                if tool_calls_buffer and self.config.mcp_support:
                    import json
                    
                    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨è¯·æ±‚ï¼‰
                    if accumulated_content.strip() or tool_calls_buffer:
                        from openai.types.chat import ChatCompletionAssistantMessageParam
                        
                        tool_calls = []
                        for tool_call in tool_calls_buffer:
                            tool_calls.append({
                                "id": tool_call["id"],
                                "type": "function",
                                "function": {
                                    "name": tool_call["function"]["name"],
                                    "arguments": tool_call["function"]["arguments"]
                                }
                            })
                        
                        assistant_message = ChatCompletionAssistantMessageParam(
                            role="assistant",
                            content=accumulated_content,
                            tool_calls=tool_calls
                        )
                        current_messages.append(assistant_message)
                    
                    # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
                    for tool_call in tool_calls_buffer:
                        tool_name = tool_call["function"]["name"]
                        try:
                            tool_args = json.loads(tool_call["function"]["arguments"] or "{}")
                            result = await self.execute_mcp_tool(tool_name, tool_args)
                            
                            # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯å†å²
                            from openai.types.chat import ChatCompletionToolMessageParam
                            
                            # ç¡®ä¿ç»“æœæ˜¯å¯åºåˆ—åŒ–çš„æ ¼å¼
                            tool_content = json.dumps(result)
                            
                            tool_result_message = ChatCompletionToolMessageParam(
                                role="tool",
                                content=tool_content,
                                tool_call_id=tool_call["id"]
                            )
                            current_messages.append(tool_result_message)
                            
                            # è¾“å‡ºç®€æ´çš„è°ƒç”¨æç¤ºç»™ç”¨æˆ·
                            tool_hint = f"<è°ƒç”¨äº† {tool_name} å·¥å…·æˆåŠŸ>"
                            generating_sentence += tool_hint
                            self.generated_text += tool_hint
                            
                        except Exception as e:
                            error_hint = f"<è°ƒç”¨ {tool_name} å·¥å…·å¤±è´¥ï¼š{e}>"
                            generating_sentence += error_hint
                            self.generated_text += error_hint
                    
                    # ç»§ç»­ä¸‹ä¸€è½®å¾ªç¯ï¼Œè®©LLMåŸºäºå·¥å…·ç»“æœç»§ç»­ç”Ÿæˆ
                    continue
                
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¾ªç¯
                break

        except asyncio.CancelledError:
            pass
        except Exception as e:
            print(repr(e))
            yield f"Someone tell the developer that there's something wrong with my AI: {repr(e)}", {
                "neutral": 1.0,
                "like": 0.0,
                "sad": 0.0,
                "disgust": 0.0,
                "anger": 0.0,
                "happy": 0.0
            }
        
        # å¤„ç†å‰©ä½™çš„å¥å­
        if generating_sentence.strip():
            yield generating_sentence.strip(), await self.get_emotion(generating_sentence)
