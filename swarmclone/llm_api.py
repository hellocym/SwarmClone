import asyncio
import os
import torch
import openai
from dataclasses import dataclass, field
from openai.types.chat import (
    ChatCompletionUserMessageParam,
    ChatCompletionAssistantMessageParam,
    ChatCompletionSystemMessageParam
)
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer
)
from .modules import *
from .messages import *
from .utils import *

@dataclass
class LLMOpenAIConfig(LLMBaseConfig):
    classifier_model_path: str = field(default="~/.swarmclone/llm/EmotionClassification/SWCBiLSTM", metadata={
        "required": False,
        "desc": "æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹è·¯å¾„"
    })
    classifier_model_id: str = field(default="MomoiaMoia/SWCBiLSTM", metadata={
        "required": False,
        "desc": "æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹id"
    })
    classifier_model_source: str = field(default="modelscope", metadata={
        "required": False,
        "desc": "æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹æ¥æºï¼Œä»…æ”¯æŒhuggingfaceæˆ–modelscope",
        "selection": True,
        "options": [
            {"key": "HuggingfaceğŸ¤—", "value": "huggingface"},
            {"key": "ModelScope", "value": "modelscope"}
        ]
    })
    model_id: str = field(default="", metadata={
        "required": True,
        "desc": "æ¨¡å‹id"
    })
    model_url: str = field(default="", metadata={
        "required": True,
        "desc": "æ¨¡å‹apiç½‘å€"
    })
    api_key: str = field(default="", metadata={
        "required": True,
        "desc": "api key"
    })
    temperature: float = field(default=0.7, metadata={
        "required": False,
        "desc": "æ¨¡å‹æ¸©åº¦",
        "selection": False,
        "options": [
            {"key": "0.7", "value": 0.7},
            {"key": "0.9", "value": 0.9},
            {"key": "1.0", "value": 1.0}
        ],
        "min": 0.0,  # æœ€å°æ¸©åº¦ä¸º 0
        "max": 1.0,  # æœ€å¤§æ¸©åº¦è®¾ä¸º 1
        "step": 0.1  # æ­¥é•¿ä¸º 0.1
    })

class LLMOpenAI(LLMBase):
    role: ModuleRoles = ModuleRoles.LLM
    config_class = LLMOpenAIConfig
    config: config_class
    def __init__(self, config: config_class | None = None, **kwargs):
        super().__init__(config, **kwargs)
        abs_classifier_path = os.path.expanduser(self.config.classifier_model_path)
        successful = False
        while not successful: # åŠ è½½æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹
            try:
                print(f"æ­£åœ¨ä»{abs_classifier_path}åŠ è½½æƒ…æ„Ÿåˆ†ç±»æ¨¡å‹â€¦â€¦")
                classifier_model = AutoModelForSequenceClassification.from_pretrained(
                    abs_classifier_path,
                    torch_dtype="auto",
                    trust_remote_code=True
                ).to("cpu")
                classifier_tokenizer = AutoTokenizer.from_pretrained(
                    abs_classifier_path,
                    padding_side="left",
                    trust_remote_code=True
                )
                successful = True
                self.classifier_model = classifier_model
                self.classifier_tokenizer = classifier_tokenizer
            except Exception:
                download_model(
                    self.config.classifier_model_id,
                    self.config.classifier_model_source,
                    abs_classifier_path
                )
        
        self.model_id = self.config.model_id
        self.client = openai.AsyncOpenAI(
            api_key=self.config.api_key,
            base_url=self.config.model_url
        )
        self.temperature = self.config.temperature
    
    @torch.no_grad()
    async def get_emotion(self, text: str) -> dict[str, float]:
        print(text)
        labels = ['neutral', 'like', 'sad', 'disgust', 'anger', 'happy']
        ids = self.classifier_tokenizer([text], return_tensors="pt")['input_ids']
        probs = (
            (await asyncio.to_thread(self.classifier_model, input_ids=ids))
            .logits
            .softmax(dim=-1)
            .squeeze()
        )
        return dict(zip(labels, probs.tolist()))
    
    def dict2message(self, message: dict[str, str]):
        match message:
            case {'role': 'user', 'content': content}:
                return ChatCompletionUserMessageParam(role="user", content=content)
            case {'role': 'assistant', 'content': content}:
                return ChatCompletionAssistantMessageParam(role="assistant", content=content)
            case {'role': 'system', 'content': content}:
                return ChatCompletionSystemMessageParam(role="system", content=content)
            case _:
                raise ValueError(f"Invalid message: {message}")
    
    async def iter_sentences_emotions(self):
        generating_sentence = ""
        try:
            response = await self.client.chat.completions.create(
                model=self.model_id,
                messages=[self.dict2message(message) for message in self.history],
                stream=True,
                temperature=self.temperature
            )
            async for delta in response:
                t = delta.choices[0].delta.content
                generating_sentence += (t or "")
                self.generated_text += (t or "")
                if (sentences := split_text(generating_sentence))[:-1]:
                    for sentence in sentences:
                        if (sentence := sentence.strip()):
                            yield sentence, await self.get_emotion(sentence)
                    generating_sentence = sentences[-1]
        finally: # è¢«ä¸­æ–­æˆ–è€…ç”Ÿæˆå®Œæ¯•
            yield generating_sentence, await self.get_emotion(generating_sentence)
