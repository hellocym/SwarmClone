import asyncio
import os
import queue
import torch
from dataclasses import dataclass, field
from transformers import (
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    AutoTokenizer,
    TextIteratorStreamer,
    StoppingCriteriaList,
    StopStringCriteria
)
from .modules import *
from .messages import *
from .utils import *

available_devices = get_devices()

@dataclass
class LLMTransformersConfig(LLMBaseConfig):
    classifier_model_path: str = field(default="~/.swarmclone/llm/EmotionClassification/SWCBiLSTM", metadata={
        "required": False,
        "desc": "ÊÉÖÊÑüÂàÜÁ±ªÊ®°ÂûãË∑ØÂæÑ"
    })
    classifier_model_id: str = field(default="MomoiaMoia/SWCBiLSTM", metadata={
        "required": False,
        "desc": "ÊÉÖÊÑüÂàÜÁ±ªÊ®°Âûãid"
    })
    classifier_model_source: str = field(default="modelscope", metadata={
        "required": False,
        "desc": "ÊÉÖÊÑüÂàÜÁ±ªÊ®°ÂûãÊù•Ê∫êÔºå‰ªÖÊîØÊåÅhuggingfaceÊàñmodelscope",
        "options": [
            {"key": "Huggingfaceü§ó", "value": "huggingface"},
            {"key": "ModelScope", "value": "modelscope"}
        ],
        "selection": True
    })
    model_path: str = field(default="~/.swarmclone/llm/MiniLM2/MiniLM2-nGPT-0.4b-instruct", metadata={
        "required": False,
        "desc": "Ê®°ÂûãË∑ØÂæÑ"
    })
    model_id: str = field(default="", metadata={
        "required": True,
        "desc": "Ê®°Âûãid"
    })
    model_source: str = field(default="modelscope", metadata={
        "required": False,
        "desc": "ËØ≠Ë®ÄÊ®°ÂûãÊù•Ê∫êÔºå‰ªÖÊîØÊåÅhuggingfaceÊàñmodelscope",
        "options": [
            {"key": "Huggingfaceü§ó", "value": "huggingface"},
            {"key": "ModelScope", "value": "modelscope"}
        ],
        "selection": True
    })
    stop_string: str = field(default="\n\n\n", metadata={
        "required": False,
        "desc": "Ê®°ÂûãËæìÂá∫ÂÅúÊ≠¢Á¨¶"
    })
    temperature: float = field(default=0.5, metadata={
        "required": False,
        "desc": "Ê®°ÂûãÊ∏©Â∫¶",
        "min": 0.0,
        "max": 1.0,
        "step": 0.1
    })
    device: str = field(default=[*available_devices.keys()][0], metadata={
        "required": False,
        "desc": "Ê®°ÂûãËøêË°åËÆæÂ§á",
        "selection": True,
        "options": [
            {"key": v, "value": k} for k, v in available_devices.items()
        ]
    })

class LLMTransformers(LLMBase):
    role: ModuleRoles = ModuleRoles.LLM
    config_class = LLMTransformersConfig
    config: config_class
    def __init__(self, config: config_class | None = None, **kwargs):
        super().__init__(config, **kwargs)
        self.config = self.config_class(**kwargs) if config is None else config
        self.stop_string = self.config.stop_string

        abs_model_path = os.path.expanduser(self.config.model_path)
        abs_classifier_path = os.path.expanduser(self.config.classifier_model_path)
        tries = 0
        while True: # Âä†ËΩΩÂ§ßËØ≠Ë®ÄÊ®°Âûã
            try:
                print(f"Ê≠£Âú®‰ªé{abs_model_path}Âä†ËΩΩËØ≠Ë®ÄÊ®°Âûã‚Ä¶‚Ä¶")
                model = AutoModelForCausalLM.from_pretrained(
                    abs_model_path,
                    torch_dtype="auto",
                    trust_remote_code=True
                ).to(self.config.device).bfloat16() # Èò≤Ê≠¢ÁàÜÂÜÖÂ≠ò
                tokenizer = AutoTokenizer.from_pretrained(
                    abs_model_path,
                    padding_side="left",
                    trust_remote_code=True
                )
                self.model = model
                self.tokenizer = tokenizer  # ÁßªÈô§Á±ªÂûãÊ≥®Ëß£‰ª•ÂÖºÂÆπÊâÄÊúâ‰ΩøÁî®Âú∫ÊôØ
                break
            except Exception:
                tries += 1
                if tries > 5:
                    raise
                download_model(self.config.model_id, self.config.model_source, abs_model_path)

        tries = 0
        while True: # Âä†ËΩΩÊÉÖÊÑüÂàÜÁ±ªÊ®°Âûã
            if tries > 5:
                raise Exception("Ê®°ÂûãÂä†ËΩΩÂ§±Ë¥•")
            try:
                print(f"Ê≠£Âú®‰ªé{abs_classifier_path}Âä†ËΩΩÊÉÖÊÑüÂàÜÁ±ªÊ®°Âûã‚Ä¶‚Ä¶")
                classifier_model = AutoModelForSequenceClassification.from_pretrained(
                    abs_classifier_path,
                    torch_dtype="auto",
                    trust_remote_code=True
                ).to("cpu")
                classifier_tokenizer = AutoTokenizer.from_pretrained(
                    abs_classifier_path,
                    padding_side="left",
                    trust_remote_code=True
                )
                self.classifier_model = classifier_model
                self.classifier_tokenizer = classifier_tokenizer
                break
            except Exception:
                tries += 1
                if tries > 5:
                    raise
                download_model(
                    self.config.classifier_model_id,
                    self.config.classifier_model_source,
                    abs_classifier_path
                )
        
        self.device: str = self.config.device
        self.temperature: float = self.config.temperature
    
    @torch.no_grad()
    async def get_emotion(self, text: str) -> dict[str, float]:
        print(text)
        labels = ['neutral', 'like', 'sad', 'disgust', 'anger', 'happy']
        ids = self.classifier_tokenizer([text], return_tensors="pt")['input_ids']
        probs = (
            (await asyncio.to_thread(self.classifier_model, input_ids=ids))
            .logits
            .softmax(dim=-1)
            .squeeze()
        )
        return dict(zip(labels, probs.tolist()))
    
    @torch.no_grad()
    async def iter_sentences_emotions(self):
        text = self.tokenizer.apply_chat_template(self.history, tokenize=False, add_generation_prompt=True)
        assert isinstance(text, str)
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=0)
        loop = asyncio.get_event_loop()
        generation_task = loop.create_task(
            asyncio.to_thread(
                self.model.generate,
                **model_inputs,
                max_new_tokens=512,
                streamer=streamer,
                stopping_criteria=StoppingCriteriaList(
                    [StopStringCriteria(self.tokenizer, self.stop_string)] if self.stop_string else []
                ),
                temperature=self.temperature
            )
        )
        generating_sentence = ""
        try:
            while True: # Á≠âÂæÖÁ¨¨‰∏Ä‰∏™tokenÈò≤Ê≠¢ÂêéÁª≠ÁîüÊàêË¢´ÈòªÂ°û
                try:
                    t: str = next(streamer)
                except queue.Empty:
                    await asyncio.sleep(0.05)
                    continue
                except StopIteration:
                    break
                generating_sentence += t
                self.generated_text += t
                if (sentences := split_text(generating_sentence))[:-1]:
                    for sentence in sentences:
                        if (sentence := sentence.strip()):
                            yield sentence, await self.get_emotion(sentence)
                    generating_sentence = sentences[-1]
        except asyncio.CancelledError:
            pass
        except Exception as e:
            print(repr(e))
            yield f"Someone tell the developer that there's something wrong with my AI: {repr(e)}", {
                "neutral": 1,
                "like": 0,
                "sad": 0,
                "disgust": 0,
                "anger": 0,
                "happy": 0
            }
        finally:
            if not generation_task.done():
                generation_task.cancel()
        yield generating_sentence, await self.get_emotion(generating_sentence)
