# By: Claude Code powered by Kimi K2
import asyncio
import os
import time
import json
import hashlib
from typing import Any
from dataclasses import dataclass, field
from contextlib import asynccontextmanager
from threading import Thread

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from pydantic import BaseModel, Field
from fastapi import FastAPI, HTTPException, Depends, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import uvicorn

from .constants import *
from .utils import *
from .modules import *

available_devices = get_devices()

# OpenAI API compatible models
class ChatCompletionMessage(BaseModel):
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: system, user, assistant, tool")
    content: str | None = Field(None, description="æ¶ˆæ¯å†…å®¹")
    name: str | None = Field(None, description="æ¶ˆæ¯å‘é€è€…åç§°")
    tool_calls: list[dict[str, Any]] | None = Field(None, description="å·¥å…·è°ƒç”¨åˆ—è¡¨")
    tool_call_id: str | None = Field(None, description="å·¥å…·è°ƒç”¨ID")

class FunctionDefinition(BaseModel):
    name: str = Field(..., description="å‡½æ•°åç§°")
    description: str = Field(..., description="å‡½æ•°æè¿°")
    parameters: dict[str, Any] = Field(..., description="å‡½æ•°å‚æ•°JSON Schema")

class ToolDefinition(BaseModel):
    type: str = Field(default="function", description="å·¥å…·ç±»å‹")
    function: FunctionDefinition = Field(..., description="å‡½æ•°å®šä¹‰")

class ChatCompletionRequest(BaseModel):
    model: str = Field(..., description="æ¨¡å‹åç§°")
    messages: list[ChatCompletionMessage] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    tools: list[ToolDefinition] | None = Field(None, description="å¯ç”¨å·¥å…·åˆ—è¡¨")
    tool_choice: str | dict[str, Any] | None = Field(None, description="å·¥å…·é€‰æ‹©ç­–ç•¥")
    temperature: float | None = Field(0.7, ge=0.0, le=2.0, description="é‡‡æ ·æ¸©åº¦")
    top_p: float | None = Field(1.0, ge=0.0, le=1.0, description="æ ¸é‡‡æ ·é˜ˆå€¼")
    max_tokens: int | None = Field(None, ge=1, description="æœ€å¤§ç”Ÿæˆtokenæ•°")
    stream: bool | None = Field(False, description="æ˜¯å¦æµå¼å“åº”")
    stop: str | list[str] | None = Field(None, description="åœæ­¢åºåˆ—")
    presence_penalty: float | None = Field(0.0, ge=-2.0, le=2.0, description="å­˜åœ¨æƒ©ç½š")
    frequency_penalty: float | None = Field(0.0, ge=-2.0, le=2.0, description="é¢‘ç‡æƒ©ç½š")

class ChatCompletionChoice(BaseModel):
    index: int
    message: ChatCompletionMessage
    finish_reason: str | None = None

class ChatCompletionUsage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: list[ChatCompletionChoice]
    usage: ChatCompletionUsage

class ChatCompletionStreamChoice(BaseModel):
    index: int
    delta: dict[str, Any]
    finish_reason: str | None = None

class ChatCompletionStreamResponse(BaseModel):
    id: str
    object: str = "chat.completion.chunk"
    created: int
    model: str
    choices: list[ChatCompletionStreamChoice]

@dataclass
class LocalLLMConfig(ModuleConfig):
    model_path: str = field(default="~/.swarmclone/local_models", metadata={
        "required": False,
        "desc": "æ¨¡å‹è·¯å¾„"
    })
    model_id: str = field(default="", metadata={
        "required": True,
        "desc": "æ¨¡å‹id"
    })
    model_source: str = field(default="modelscope", metadata={
        "required": False,
        "desc": "è¯­è¨€æ¨¡å‹æ¥æºï¼Œä»…æ”¯æŒhuggingfaceæˆ–modelscope",
        "options": [
            {"key": "HuggingfaceğŸ¤—", "value": "huggingface"},
            {"key": "ModelScope", "value": "modelscope"}
        ],
        "selection": True
    })
    stop_string: str = field(default="\n\n\n", metadata={
        "required": False,
        "desc": "æ¨¡å‹è¾“å‡ºåœæ­¢ç¬¦"
    })
    temperature: float = field(default=0.5, metadata={
        "required": False,
        "desc": "æ¨¡å‹æ¸©åº¦",
        "min": 0.0,
        "max": 1.0,
        "step": 0.1
    })
    device: str = field(default=[*available_devices.keys()][0], metadata={
        "required": False,
        "desc": "æ¨¡å‹è¿è¡Œè®¾å¤‡",
        "selection": True,
        "options": [
            {"key": v, "value": k} for k, v in available_devices.items()
        ]
    })
    host: str = field(default="127.0.0.1", metadata={
        "required": False,
        "desc": "æœåŠ¡å™¨ç›‘å¬åœ°å€"
    })
    port: int = field(default=9000, metadata={
        "required": False,
        "desc": "æœåŠ¡å™¨ç«¯å£",
        "min": 1024,
        "max": 65535
    })

class LocalLLM(ModuleBase):
    role: ModuleRoles = ModuleRoles.PLUGIN
    config_class = LocalLLMConfig
    config: config_class
    
    def __init__(self, config: config_class | None = None, **kwargs: Any):
        super().__init__(config, **kwargs)
        self.app = None
        self.server_task = None
        
        abs_model_dir_path = os.path.expanduser(self.config.model_path)
        abs_model_path = os.path.join(abs_model_dir_path, hashlib.md5(self.config.model_id.encode()).hexdigest())
        tries = 0
        while True:
            try:
                print(f"æ­£åœ¨ä»{abs_model_path}åŠ è½½è¯­è¨€æ¨¡å‹â€¦â€¦")
                model = AutoModelForCausalLM.from_pretrained(
                    abs_model_path,
                    torch_dtype="auto",
                    trust_remote_code=True
                ).to(self.config.device).bfloat16()
                tokenizer = AutoTokenizer.from_pretrained(
                    abs_model_path,
                    padding_side="left",
                    trust_remote_code=True
                )
                self.model = model
                self.tokenizer = tokenizer
                
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                break
            except Exception as e:
                tries += 1
                if tries > 5:
                    raise e
                download_model(self.config.model_id, self.config.model_source, abs_model_path)

    def _create_prompt_with_template(self, messages: list[ChatCompletionMessage], tools: list[ToolDefinition] | None = None) -> str:
        """ä½¿ç”¨æ¨¡å‹çš„chat templateç”Ÿæˆæç¤ºï¼Œå¦‚æœæ¨¡å‹ä¸æ”¯æŒtool callingåˆ™å¿½ç•¥å·¥å…·"""
        # å°†æ¶ˆæ¯è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        conversation = []
        for msg in messages:
            message_dict: dict[str, Any] = {
                "role": msg.role,
                "content": msg.content or ""
            }
            if msg.name:
                message_dict["name"] = msg.name
            if msg.tool_calls and msg.role == "assistant":
                message_dict["tool_calls"] = msg.tool_calls
            if msg.tool_call_id and msg.role == "tool":
                message_dict["tool_call_id"] = msg.tool_call_id
            conversation.append(message_dict)
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒtoolså‚æ•°
        supports_tools = False
        try:
            # æµ‹è¯•tokenizeræ˜¯å¦æ”¯æŒtoolså‚æ•°
            test_tools = [{"type": "function", "function": {"name": "test", "description": "test", "parameters": {}}}]
            self.tokenizer.apply_chat_template(
                [{"role": "user", "content": "test"}], 
                tools=test_tools, 
                add_generation_prompt=True,
                tokenize=False
            )
            supports_tools = True
        except Exception:
            supports_tools = False
        
        # æ ¹æ®æ”¯æŒæƒ…å†µå†³å®šæ˜¯å¦ä½¿ç”¨tools
        if tools and supports_tools:
            tools_dict = []
            for tool in tools:
                tools_dict.append({
                    "type": tool.type,
                    "function": {
                        "name": tool.function.name,
                        "description": tool.function.description,
                        "parameters": tool.function.parameters
                    }
                })
            prompt = self.tokenizer.apply_chat_template(
                conversation,
                tools=tools_dict,
                add_generation_prompt=True,
                tokenize=False
            )
        else:
            # æ¨¡å‹ä¸æ”¯æŒtoolsï¼Œå¿½ç•¥æ‰€æœ‰å·¥å…·å‚æ•°
            prompt = self.tokenizer.apply_chat_template(
                conversation,
                add_generation_prompt=True,
                tokenize=False
            )
        
        return prompt

    def _count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        return len(self.tokenizer.encode(text))

    async def _generate_response(
        self, 
        messages: list[ChatCompletionMessage], 
        temperature: float = 0.7,
        max_tokens: int | None = None,
        stop: str | list[str] | None = None,
        tools: list[ToolDefinition] | None = None
    ) -> tuple[str, list[dict[str, Any]] | None]:
        """ç”Ÿæˆæ¨¡å‹å“åº”ï¼Œè¿”å›(å†…å®¹,å·¥å…·è°ƒç”¨åˆ—è¡¨)"""
        prompt = self._create_prompt_with_template(messages, tools)
        inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.config.device)
        
        max_new_tokens = max_tokens or 512
        stop_strings = []
        if stop:
            if isinstance(stop, str):
                stop_strings = [stop]
            else:
                stop_strings = stop
        if self.config.stop_string:
            stop_strings.append(self.config.stop_string)

        # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œé˜»å¡æ“ä½œ
        def _generate_sync():
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    tokenizer=self.tokenizer
                )
            
            response = self.tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
            
            # å¤„ç†åœæ­¢ç¬¦
            for stop_str in stop_strings:
                if stop_str in response:
                    response = response[:response.find(stop_str)]
            
            return response.strip()

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œé˜»å¡è°ƒç”¨
        response = await asyncio.get_event_loop().run_in_executor(None, _generate_sync)
        
        # ç”±äºä½¿ç”¨chat templateï¼Œå·¥å…·è°ƒç”¨ç”±æ¨¡å‹ç›´æ¥å¤„ç†
        tool_calls = None
        
        return response.strip(), tool_calls

    async def _generate_stream(
        self, 
        messages: list[ChatCompletionMessage], 
        temperature: float = 0.7,
        max_tokens: int | None = None,
        stop: str | list[str] | None = None,
        tools: list[ToolDefinition] | None = None
    ):
        """æµå¼ç”Ÿæˆå“åº”"""
        prompt = self._create_prompt_with_template(messages, tools)
        inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.config.device)
        
        max_new_tokens = max_tokens or 512
        stop_strings = []
        if stop:
            if isinstance(stop, str):
                stop_strings = [stop]
            else:
                stop_strings = stop
        if self.config.stop_string:
            stop_strings.append(self.config.stop_string)

        # ä½¿ç”¨transformersæµå¼API
        streamer = TextIteratorStreamer(
            self.tokenizer,
            skip_prompt=True,
            skip_special_tokens=True
        )

        # å‡†å¤‡ç”Ÿæˆå‚æ•°
        generation_kwargs = {
            "input_ids": inputs,
            "max_new_tokens": max_new_tokens,
            "temperature": temperature,
            "do_sample": True,
            "pad_token_id": self.tokenizer.eos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "streamer": streamer,
            "tokenizer": self.tokenizer
        }

        # å¦‚æœæœ‰åœæ­¢å­—ç¬¦ä¸²ï¼Œæ·»åŠ åˆ°ç”Ÿæˆå‚æ•°
        if stop_strings:
            generation_kwargs["stop_strings"] = stop_strings

        def _generate_with_streamer():
            with torch.no_grad():
                self.model.generate(**generation_kwargs)

        # åœ¨åå°çº¿ç¨‹å¯åŠ¨ç”Ÿæˆ
        generation_thread = Thread(target=_generate_with_streamer)
        generation_thread.start()

        # è·å–æµå¼è¾“å‡º
        for token in streamer:
            yield token

    def _create_app(self) -> FastAPI:
        """åˆ›å»ºFastAPIåº”ç”¨"""
        security = HTTPBearer()

        async def verify_api_key(credentials: HTTPAuthorizationCredentials = Depends(security)):
            """å ä½å¼API Keyé‰´æƒï¼šæ¥å—ä»»æ„API Keyå­—ç¬¦ä¸²"""
            # å ä½å¼é‰´æƒï¼Œæ¥å—ä»»æ„éç©ºå­—ç¬¦ä¸²
            if not credentials.credentials or not credentials.credentials.strip():
                raise HTTPException(status_code=401, detail="API Keyä¸èƒ½ä¸ºç©º")
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ—¥å¿—è®°å½•ï¼Œä½†å§‹ç»ˆè¿”å›True
            return True

        @asynccontextmanager
        async def lifespan(_app: FastAPI):
            print(f"Local LLM server starting on {self.config.host}:{self.config.port}")
            yield
            print("Local LLM server shutting down")

        app = FastAPI(
            title="Local LLM API",
            description="OpenAI Chat Completion API compatible local LLM server",
            version="1.0.0",
            lifespan=lifespan
        )

        # æ·»åŠ CORSä¸­é—´ä»¶
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )

        @app.get("/")
        async def root():
            return {"message": "Local LLM API Server", "model": self.config.model_id}

        @app.get("/models")
        async def list_models():
            return {
                "object": "list",
                "data": [
                    {
                        "id": self.config.model_id,
                        "object": "model",
                        "created": int(time.time()),
                        "owned_by": "local"
                    }
                ]
            }

        @app.post("/v1/chat/completions")
        async def chat_completions(request: ChatCompletionRequest, auth: bool = Depends(verify_api_key)):
            try:
                if request.stream:
                    return StreamingResponse(
                        self._stream_response(request),
                        media_type="text/event-stream"
                    )
                else:
                    return await self._sync_response(request)
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))

        @app.get("/health")
        async def health_check(auth: bool = Depends(verify_api_key)):
            return {"status": "healthy", "model_loaded": True}

        return app

    async def _sync_response(self, request: ChatCompletionRequest) -> ChatCompletionResponse:
        """åŒæ­¥å“åº”å¤„ç†"""
        response_text, tool_calls = await self._generate_response(
            messages=request.messages,
            temperature=request.temperature or 0.7,
            max_tokens=request.max_tokens,
            stop=request.stop,
            tools=request.tools
        )

        prompt_tokens = sum(self._count_tokens(msg.content or "") for msg in request.messages)
        completion_tokens = self._count_tokens(response_text or "")

        # ç¡®å®šå®ŒæˆåŸå› 
        finish_reason = "tool_calls" if tool_calls else "stop"

        return ChatCompletionResponse(
            id=f"chatcmpl-{int(time.time())}",
            created=int(time.time()),
            model=request.model,
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=ChatCompletionMessage(
                        role="assistant", 
                        content=response_text or None, 
                        name=None, 
                        tool_calls=tool_calls, 
                        tool_call_id=None
                    ),
                    finish_reason=finish_reason
                )
            ],
            usage=ChatCompletionUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens
            )
        )

    async def _stream_response(self, request: ChatCompletionRequest):
        """æµå¼å“åº”å¤„ç†"""
        response_id = f"chatcmpl-{int(time.time())}"
        created = int(time.time())

        # å‘é€å¼€å§‹æ¶ˆæ¯
        start_data = {
            'id': response_id,
            'object': 'chat.completion.chunk',
            'created': created,
            'model': request.model,
            'choices': [{'index': 0, 'delta': {'role': 'assistant'}, 'finish_reason': None}]
        }
        yield f"data: {json.dumps(start_data)}\n\n"

        # æµå¼ç”Ÿæˆå“åº”
        response_text = ""
        async for chunk in self._generate_stream(
            messages=request.messages,
            temperature=request.temperature or 0.7,
            max_tokens=request.max_tokens,
            stop=request.stop,
            tools=request.tools
        ):
            response_text += chunk
            if chunk.strip():  # åªå‘é€éç©ºç‰‡æ®µ
                chunk_data = {
                    'id': response_id,
                    'object': 'chat.completion.chunk',
                    'created': created,
                    'model': request.model,
                    'choices': [{'index': 0, 'delta': {'content': chunk}, 'finish_reason': None}]
                }
                yield f"data: {json.dumps(chunk_data)}\n\n"

        # å‘é€ç»“æŸæ¶ˆæ¯
        finish_reason = "stop"
        end_data = {
            'id': response_id,
            'object': 'chat.completion.chunk',
            'created': created,
            'model': request.model,
            'choices': [{'index': 0, 'delta': {}, 'finish_reason': finish_reason}]
        }
        yield f"data: {json.dumps(end_data)}\n\n"
        yield "data: [DONE]\n\n"

    async def start_server(self):
        """å¯åŠ¨æœåŠ¡å™¨"""
        if self.server_task and not self.server_task.done():
            print("æœåŠ¡å™¨å·²åœ¨è¿è¡Œä¸­")
            return

        self.app = self._create_app()
        config = uvicorn.Config(
            self.app,
            host=self.config.host,
            port=self.config.port,
            log_level="info",
            access_log=False
        )
        server = uvicorn.Server(config)
        
        # åœ¨å½“å‰äº‹ä»¶å¾ªç¯ä¸­è¿è¡ŒæœåŠ¡å™¨
        self.server_task = asyncio.create_task(server.serve())
        print(f"Local LLM server started at http://{self.config.host}:{self.config.port}")

    async def stop_server(self):
        """åœæ­¢æœåŠ¡å™¨"""
        if self.server_task and not self.server_task.done():
            self.server_task.cancel()
            try:
                await self.server_task
            except asyncio.CancelledError:
                pass
            print("Local LLM server stopped")

    async def run(self):
        """æ¨¡å—ä¸»è¿è¡Œå‡½æ•°"""
        await self.start_server()
        # ä¿æŒæ¨¡å—è¿è¡Œ
        try:
            while True:
                await asyncio.sleep(1)
        except asyncio.CancelledError:
            await self.stop_server()
            raise
